{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aptos2019-blindness-detection', 'aptos20192', 'aptocsubbmition']\n",
      "['test.csv', 'test_images', 'train.csv', 'sample_submission.csv', 'train_images']\n",
      "['mysubmisson.csv']\n",
      "['mean_v4.npy', 'model.model', 'label_dict_v4.pickle']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "print(os.listdir(\"../input/aptos2019-blindness-detection\"))\n",
    "print(os.listdir(\"../input/aptocsubbmition\"))\n",
    "print(os.listdir(\"../input/aptos20192\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import six\n",
    "import time \n",
    "\n",
    "import warnings \n",
    "import io\n",
    "import cgi\n",
    "import cgitb\n",
    " \n",
    "import sys\n",
    "import codecs\n",
    "import locale \n",
    "cgitb.enable()\n",
    "import cupy as np\n",
    "import pandas as pd\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "import pickle \n",
    "import re\n",
    "import json\n",
    "import cv2\n",
    "import io\n",
    "import scipy.misc\n",
    "import base64\n",
    "import numpy \n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import initializers\n",
    "import chainer.links as L\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNetBN(chainer.Chain):\n",
    "\n",
    "    \"\"\"New GoogLeNet of BatchNormalization version.\"\"\"\n",
    "\n",
    "    insize = 224\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GoogLeNetBN, self).__init__(\n",
    "            conv1=L.Convolution2D(None, 64, 7, stride=2, pad=3, nobias=True),\n",
    "            norm1=L.BatchNormalization(64),\n",
    "            conv2=L.Convolution2D(None, 192, 3, pad=1, nobias=True),\n",
    "            norm2=L.BatchNormalization(192),\n",
    "            inc3a=L.InceptionBN(None, 64, 64, 64, 64, 96, 'avg', 32),\n",
    "            inc3b=L.InceptionBN(None, 64, 64, 96, 64, 96, 'avg', 64),\n",
    "            inc3c=L.InceptionBN(None, 0, 128, 160, 64, 96, 'max', stride=2),\n",
    "            inc4a=L.InceptionBN(None, 224, 64, 96, 96, 128, 'avg', 128),\n",
    "            inc4b=L.InceptionBN(None, 192, 96, 128, 96, 128, 'avg', 128),\n",
    "            inc4c=L.InceptionBN(None, 128, 128, 160, 128, 160, 'avg', 128),\n",
    "            inc4d=L.InceptionBN(None, 64, 128, 192, 160, 192, 'avg', 128),\n",
    "            inc4e=L.InceptionBN(None, 0, 128, 192, 192, 256, 'max', stride=2),\n",
    "            inc5a=L.InceptionBN(None, 352, 192, 320, 160, 224, 'avg', 128),\n",
    "            inc5b=L.InceptionBN(None, 352, 192, 320, 192, 224, 'max', 128),\n",
    "            out=L.Linear(None, 1000),\n",
    "\n",
    "            conva=L.Convolution2D(None, 128, 1, nobias=True),\n",
    "            norma=L.BatchNormalization(128),\n",
    "            lina=L.Linear(None, 1024, nobias=True),\n",
    "            norma2=L.BatchNormalization(1024),\n",
    "            outa=L.Linear(None, 1000),\n",
    "\n",
    "            convb=L.Convolution2D(None, 128, 1, nobias=True),\n",
    "            normb=L.BatchNormalization(128),\n",
    "            linb=L.Linear(None, 1024, nobias=True),\n",
    "            normb2=L.BatchNormalization(1024),\n",
    "            outb=L.Linear(None, 1000),\n",
    "        )\n",
    "        self._train = True\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "\n",
    "    @train.setter\n",
    "    def train(self, value):\n",
    "        self._train = value\n",
    "        self.inc3a.train = value\n",
    "        self.inc3b.train = value\n",
    "        self.inc3c.train = value\n",
    "        self.inc4a.train = value\n",
    "        self.inc4b.train = value\n",
    "        self.inc4c.train = value\n",
    "        self.inc4d.train = value\n",
    "        self.inc4e.train = value\n",
    "        self.inc5a.train = value\n",
    "        self.inc5b.train = value\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "\n",
    "        h = F.max_pooling_2d(\n",
    "            F.relu(self.norm1(self.conv1(x))), 3, stride=2, pad=1)\n",
    "        h = F.max_pooling_2d(\n",
    "            F.relu(self.norm2(self.conv2(h))), 3, stride=2, pad=1)\n",
    "\n",
    "        h = self.inc3a(h)\n",
    "        h = self.inc3b(h)\n",
    "        h = self.inc3c(h)\n",
    "        h = self.inc4a(h)\n",
    "\n",
    "        a = F.average_pooling_2d(h, 5, stride=3)\n",
    "        a = F.relu(self.norma(self.conva(a)))\n",
    "        a = F.relu(self.norma2(self.lina(a)))\n",
    "        a = self.outa(a)\n",
    "        loss1 = F.softmax_cross_entropy(a, t)\n",
    "\n",
    "        h = self.inc4b(h)\n",
    "        h = self.inc4c(h)\n",
    "        h = self.inc4d(h)\n",
    "\n",
    "        b = F.average_pooling_2d(h, 5, stride=3)\n",
    "        b = F.relu(self.normb(self.convb(b)))\n",
    "        b = F.relu(self.normb2(self.linb(b)))\n",
    "        b = self.outb(b)\n",
    "        loss2 = F.softmax_cross_entropy(b, t)\n",
    "\n",
    "        h = self.inc4e(h)\n",
    "        h = self.inc5a(h)\n",
    "        h = F.average_pooling_2d(self.inc5b(h), 7)\n",
    "        h = self.out(h)\n",
    "        loss3 = F.softmax_cross_entropy(h, t)\n",
    "\n",
    "        loss = 0.3 * (loss1 + loss2) + loss3\n",
    "        accuracy = F.accuracy(h, t)\n",
    "\n",
    "        chainer.report({\n",
    "            'loss': loss,\n",
    "            'loss1': loss1,\n",
    "            'loss2': loss2,\n",
    "            'loss3': loss3,\n",
    "            'accuracy': accuracy,\n",
    "        }, self)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        with chainer.no_backprop_mode(),chainer.using_config('train',False):\n",
    "            h = F.max_pooling_2d(\n",
    "                F.relu(self.norm1(self.conv1(x))), 3, stride=2, pad=1)\n",
    "            h = F.max_pooling_2d(\n",
    "                F.relu(self.norm2(self.conv2(h))), 3, stride=2, pad=1)\n",
    "\n",
    "            h = self.inc3a(h)\n",
    "            h = self.inc3b(h)\n",
    "            h = self.inc3c(h)\n",
    "            h = self.inc4a(h)\n",
    "\n",
    "            # a = F.average_pooling_2d(h, 5, stride=3)\n",
    "            # a = F.relu(self.norma(self.conva(a)))\n",
    "            # a = F.relu(self.norma2(self.lina(a)))\n",
    "            # a = self.outa(a)\n",
    "            # loss1 = F.softmax_cross_entropy(a, t)\n",
    "\n",
    "            h = self.inc4b(h)\n",
    "            h = self.inc4c(h)\n",
    "            h = self.inc4d(h)\n",
    "\n",
    "            # b = F.average_pooling_2d(h, 5, stride=3)\n",
    "            # b = F.relu(self.normb(self.convb(b)))\n",
    "            # b = F.relu(self.normb2(self.linb(b)))\n",
    "            # b = self.outb(b)\n",
    "            # loss2 = F.softmax_cross_entropy(b, t)\n",
    "\n",
    "            h = self.inc4e(h)\n",
    "            h = self.inc5a(h)\n",
    "            h = F.average_pooling_2d(self.inc5b(h), 7)\n",
    "            h = self.out(h)\n",
    "\n",
    "            return F.softmax(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************\n",
      "Load models\n",
      "Model defined elapsed time(経過時間) : 5.325917482376099[sec]\n",
      "end:130.75533962249756[sec]\n"
     ]
    }
   ],
   "source": [
    "UNK = 0\n",
    "EOS = 1\n",
    "class load_model():\n",
    "    def main():\n",
    "        model = GoogLeNetBN()\n",
    "        serializers.load_npz(\"../input/aptos20192/model.model\", model)\n",
    "        return model \n",
    "class model_define():\n",
    "    start = time.time()\n",
    "    print(\"****************************************************************\")\n",
    "    print(\"Load models\")\n",
    "    \n",
    "    model1 = load_model.main()    \n",
    "    chainer.serializers.save_npz('model2.model', model1)\n",
    "    \n",
    "    print(\"Model defined elapsed time(経過時間) : {0}\".format(time.time() - start) + \"[sec]\")\n",
    "\n",
    "def pred():\n",
    "    start = time.time() \n",
    "    submit_list = [['id_code','diagnosis']]\n",
    "    df_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv').values.tolist()\n",
    "    with open('../input/aptos20192/label_dict_v4.pickle','rb') as f:\n",
    "        categories = pickle.load(f)\n",
    "    mean_image =  np.load('../input/aptos20192/mean_v4.npy') \n",
    "    model = model_define.model1\n",
    "    cropwidth = 256 - model.insize\n",
    "    size = (256, 256) \n",
    "    for item in df_test:\n",
    "        img = imread('../input/aptos2019-blindness-detection/test_images/' + item[0] + '.png') \n",
    "        #Create Half Size Image\n",
    "        #halfImg = cv2.resize(img, (orgHeight / 2, orgWidth / 2))\n",
    "        image = cv2.resize(img, size)\n",
    "        image = np.asarray(image).transpose(2, 0, 1)\n",
    "\n",
    "        \n",
    "        top = random.randint(0, cropwidth - 1)\n",
    "        left = random.randint(0, cropwidth - 1)\n",
    "        bottom = model.insize + top\n",
    "        right = model.insize + left\n",
    "        image = image[:, top:bottom, left:right].astype(np.float32)\n",
    "        image -= mean_image[:, top:bottom, left:right]\n",
    "        image /= 255\n",
    "            \n",
    "        model.to_gpu()  \n",
    "\n",
    "        x = np.ndarray(\n",
    "                (1, 3, model.insize, model.insize), dtype=np.float32)\n",
    "        x[0]=image  \n",
    "        x = chainer.Variable(x)\n",
    "        score = model.predict(x) \n",
    "\n",
    "\n",
    "        top_k =10\n",
    "        prediction = list(zip(score.data[0].tolist(), [v for k,v in categories.items()]))\n",
    "        prediction= sorted(prediction,reverse=True)\n",
    "        submit_list.append([item[0],prediction[0][1]]) \n",
    " \n",
    "    with open('submission.csv', 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく \n",
    "        writer.writerows(submit_list) # 2次元配列も書き込める  \n",
    "    print(\"end:{0}\".format(time.time()-start) +'[sec]')\n",
    "if __name__ == \"__main__\":\n",
    "    model_define()\n",
    "    pred()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
