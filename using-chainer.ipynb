{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy\n",
      "\u001b[J1           1.01903e+08                        0.502502                                 \n",
      "\u001b[J2           8.88209e+07                        0.531692                                 \n",
      "\u001b[J3           2.62078e+08                        0.547198                                 \n",
      "\u001b[J4           6.44799e+08                        0.551514                                 \n",
      "\u001b[J5           1.39459e+09                        0.554123                                 \n",
      "\u001b[J6           1.77717e+09                        0.553849                                 \n",
      "\u001b[J7           2.26297e+09                        0.554112                                 \n",
      "\u001b[J8           1.0153e+09                        0.556173                                 \n",
      "\u001b[J9           2.71985e+09                        0.556888                                 \n",
      "\u001b[J10          2.00022e+09                        0.549765                                 \n",
      "\u001b[J11          4.17282e+09                        0.545653                                 \n",
      "\u001b[J12          6.06083e+09                        0.543358                                 \n",
      "\u001b[J13          3.90356e+09                        0.545091                                 \n",
      "\u001b[J14          4.54928e+09                        0.549418                                 \n",
      "\u001b[J15          4.05495e+09                        0.547908                                 \n",
      "\u001b[J16          4.35727e+09                        0.542401                                 \n",
      "\u001b[J17          2.97379e+09                        0.548219                                 \n",
      "\u001b[J18          3.60815e+09                        0.546358                                 \n",
      "\u001b[J19          2.87984e+09                        0.544692                                 \n",
      "\u001b[J20          2.23389e+09                        0.547773                                 \n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import argparse\n",
    "import collections\n",
    "import cupy as xp\n",
    "import numpy as np\n",
    "import six\n",
    "\n",
    "import chainer\n",
    "from chainer.backends import cuda\n",
    "import chainer.functions as F\n",
    "import chainer.initializers as I\n",
    "import chainer.links as L\n",
    "import chainer.optimizers as O\n",
    "from chainer import reporter\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='cupy')\n",
    "from gensim import corpora\n",
    "dtypes = {\n",
    "        'MachineIdentifier':                                    'category',\n",
    "        'ProductName':                                          'category',\n",
    "        'EngineVersion':                                        'category',\n",
    "        'AppVersion':                                           'category',\n",
    "        'AvSigVersion':                                         'category',\n",
    "        'IsBeta':                                               'int8',\n",
    "        'RtpStateBitfield':                                     'float16',\n",
    "        'IsSxsPassiveMode':                                     'int8',\n",
    "        'DefaultBrowsersIdentifier':                            'float16',\n",
    "        'AVProductStatesIdentifier':                            'float32',\n",
    "        'AVProductsInstalled':                                  'float16',\n",
    "        'AVProductsEnabled':                                    'float16',\n",
    "        'HasTpm':                                               'int8',\n",
    "        'CountryIdentifier':                                    'int16',\n",
    "        'CityIdentifier':                                       'float32',\n",
    "        'OrganizationIdentifier':                               'float16',\n",
    "        'GeoNameIdentifier':                                    'float16',\n",
    "        'LocaleEnglishNameIdentifier':                          'int8',\n",
    "        'Platform':                                             'category',\n",
    "        'Processor':                                            'category',\n",
    "        'OsVer':                                                'category',\n",
    "        'OsBuild':                                              'int16',\n",
    "        'OsSuite':                                              'int16',\n",
    "        'OsPlatformSubRelease':                                 'category',\n",
    "        'OsBuildLab':                                           'category',\n",
    "        'SkuEdition':                                           'category',\n",
    "        'IsProtected':                                          'float16',\n",
    "        'AutoSampleOptIn':                                      'int8',\n",
    "        'PuaMode':                                              'category',\n",
    "        'SMode':                                                'float16',\n",
    "        'IeVerIdentifier':                                      'float16',\n",
    "        'SmartScreen':                                          'category',\n",
    "        'Firewall':                                             'float16',\n",
    "        'UacLuaenable':                                         'float32',\n",
    "        'Census_MDC2FormFactor':                                'category',\n",
    "        'Census_DeviceFamily':                                  'category',\n",
    "        'Census_OEMNameIdentifier':                             'float16',\n",
    "        'Census_OEMModelIdentifier':                            'float32',\n",
    "        'Census_ProcessorCoreCount':                            'float16',\n",
    "        'Census_ProcessorManufacturerIdentifier':               'float16',\n",
    "        'Census_ProcessorModelIdentifier':                      'float16',\n",
    "        'Census_ProcessorClass':                                'category',\n",
    "        'Census_PrimaryDiskTotalCapacity':                      'float32',\n",
    "        'Census_PrimaryDiskTypeName':                           'category',\n",
    "        'Census_SystemVolumeTotalCapacity':                     'float32',\n",
    "        'Census_HasOpticalDiskDrive':                           'int8',\n",
    "        'Census_TotalPhysicalRAM':                              'float32',\n",
    "        'Census_ChassisTypeName':                               'category',\n",
    "        'Census_InternalPrimaryDiagonalDisplaySizeInInches':    'float16',\n",
    "        'Census_InternalPrimaryDisplayResolutionHorizontal':    'float16',\n",
    "        'Census_InternalPrimaryDisplayResolutionVertical':      'float16',\n",
    "        'Census_PowerPlatformRoleName':                         'category',\n",
    "        'Census_InternalBatteryType':                           'category',\n",
    "        'Census_InternalBatteryNumberOfCharges':                'float32',\n",
    "        'Census_OSVersion':                                     'category',\n",
    "        'Census_OSArchitecture':                                'category',\n",
    "        'Census_OSBranch':                                      'category',\n",
    "        'Census_OSBuildNumber':                                 'int16',\n",
    "        'Census_OSBuildRevision':                               'int32',\n",
    "        'Census_OSEdition':                                     'category',\n",
    "        'Census_OSSkuName':                                     'category',\n",
    "        'Census_OSInstallTypeName':                             'category',\n",
    "        'Census_OSInstallLanguageIdentifier':                   'float16',\n",
    "        'Census_OSUILocaleIdentifier':                          'int16',\n",
    "        'Census_OSWUAutoUpdateOptionsName':                     'category',\n",
    "        'Census_IsPortableOperatingSystem':                     'int8',\n",
    "        'Census_GenuineStateName':                              'category',\n",
    "        'Census_ActivationChannel':                             'category',\n",
    "        'Census_IsFlightingInternal':                           'float16',\n",
    "        'Census_IsFlightsDisabled':                             'float16',\n",
    "        'Census_FlightRing':                                    'category',\n",
    "        'Census_ThresholdOptIn':                                'float16',\n",
    "        'Census_FirmwareManufacturerIdentifier':                'float16',\n",
    "        'Census_FirmwareVersionIdentifier':                     'float32',\n",
    "        'Census_IsSecureBootEnabled':                           'int8',\n",
    "        'Census_IsWIMBootEnabled':                              'float16',\n",
    "        'Census_IsVirtualDevice':                               'float16',\n",
    "        'Census_IsTouchEnabled':                                'int8',\n",
    "        'Census_IsPenCapable':                                  'int8',\n",
    "        'Census_IsAlwaysOnAlwaysConnectedCapable':              'float16',\n",
    "        'Wdft_IsGamer':                                         'float16',\n",
    "        'Wdft_RegionIdentifier':                                'float16',\n",
    "        'HasDetections':                                        'int8'\n",
    "        }\n",
    "class Parallel_process_source_dic():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # ここを並列処理する\n",
    "    def func(document, argument1):\n",
    "        wrk_source = list(map(argument1.get, document))\n",
    "        return wrk_source\n",
    "\n",
    "    def wrapper(args):\n",
    "        return Parallel_process_source_dic.func(*args)\n",
    "\n",
    "    def multi_process(proc_list):\n",
    "        # プロセス数:8(8個のcpuで並列処理)\n",
    "        p = Pool(6)\n",
    "        output = p.map(Parallel_process_source_dic.wrapper, proc_list)\n",
    "        # プロセスの終了\n",
    "        p.close()\n",
    "        return output\n",
    "\n",
    "    def main_proc_1(source, dictionary):\n",
    "        proc_list = [(item, dictionary) for item in source]\n",
    "\n",
    "        output = Parallel_process_source_dic.multi_process(proc_list)\n",
    "        return output\n",
    "class WindowIterator(chainer.dataset.Iterator):\n",
    "    \"\"\"Dataset iterator to create a batch of sequences at different positions.\n",
    "\n",
    "    This iterator returns a pair of the current words and the context words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 small_str_source=None,\n",
    "                 small_int_source=None,\n",
    "                 big_str_source = None,\n",
    "                 big_int_source = None,\n",
    "                 float_source=None,\n",
    "                 ans=None,\n",
    "                batch_size=None, \n",
    "                repeat=True): \n",
    "        self.small_str_source = small_str_source\n",
    "        self.small_int_source = small_int_source\n",
    "        self.big_str_source = big_str_source\n",
    "        self.big_int_source = big_int_source\n",
    "        self.float_source = float_source\n",
    "        self.ans = ans \n",
    "        self.batch_size = batch_size\n",
    "        self._repeat = repeat\n",
    "        # order is the array which is shuffled ``[window, window + 1, ...,\n",
    "        # len(dataset) - window - 1]``\n",
    "        \n",
    "        self.current_position = 0\n",
    "        # Number of completed sweeps over the dataset. In this case, it is\n",
    "        # incremented if every word is visited at least once after the last\n",
    "        # increment.\n",
    "        self.epoch = 0\n",
    "        # True if the epoch is incremented at the last iteration.\n",
    "        self.is_new_epoch = False\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"This iterator returns a list representing a mini-batch.\n",
    "\n",
    "        Each item indicates a different position in the original sequence.\n",
    "        \"\"\"\n",
    "        if not self._repeat and self.epoch > 0:\n",
    "            raise StopIteration\n",
    "\n",
    "        i = self.current_position\n",
    "        i_end = i + self.batch_size\n",
    "        mini_small_data_str = self.small_str_source[i:i_end]\n",
    "        mini_small_data_int = self.small_int_source[i:i_end]\n",
    "        mini_big_data_str = self.big_str_source[:,i:i_end]\n",
    "        mini_big_data_int = self.big_int_source[:,i:i_end]\n",
    "        mini_data_float = self.float_source[i:i_end]\n",
    "        ans_data = self.ans[i:i_end] \n",
    "        \n",
    "        if i_end >= len(self.small_str_source):\n",
    "            self.epoch += 1\n",
    "            self.is_new_epoch = True\n",
    "            self.current_position = 0\n",
    "        else:\n",
    "            self.is_new_epoch = False\n",
    "            self.current_position = i_end\n",
    "\n",
    "        return mini_small_data_str,mini_small_data_int,mini_big_data_str,mini_big_data_int,mini_data_float,ans_data\n",
    "\n",
    "    @property\n",
    "    def epoch_detail(self):\n",
    "        return self.epoch + float(self.current_position) / len(self.small_str_source)\n",
    "\n",
    "    def serialize(self, serializer):\n",
    "        self.current_position = serializer('current_position',\n",
    "                                           self.current_position)\n",
    "        self.epoch = serializer('epoch', self.epoch)\n",
    "        self.is_new_epoch = serializer('is_new_epoch', self.is_new_epoch)\n",
    "        if self._order is not None:\n",
    "            serializer('_order', self._order)\n",
    "def convert(batch, device):\n",
    "    mini_small_data_str,mini_small_data_int,mini_big_data_str,mini_big_data_int,mini_data_float,ans_data = batch\n",
    "    #if device >= 0:\n",
    "    #    mini_data_str = cuda.to_gpu(mini_data_str)\n",
    "    #    mini_data_int = cuda.to_gpu(mini_data_float)\n",
    "    #    mini_data_float = cuda.to_gpu(mini_data_float)\n",
    "    #    ans_data = cuda.to_gpu(ans_data) \n",
    "    return mini_small_data_str,mini_small_data_int,mini_big_data_str,mini_big_data_int,mini_data_float,ans_data\n",
    "\n",
    "class ms_malware(Chain):\n",
    "    \"\"\"Definition of Skip-gram Model\"\"\" \n",
    "    def __init__(self, \n",
    "                 small_str_source_len,\n",
    "                 small_int_source_len,\n",
    "                 big_str_source_len,\n",
    "                 big_int_source_len,\n",
    "                 float_source_len,\n",
    "                 small_str_dictionary,\n",
    "                 small_int_dictionary,\n",
    "                 big_str_dictionary,\n",
    "                 big_int_dictionary):\n",
    "        super(ms_malware, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.small_embed_str = L.EmbedID(int(len(small_str_dictionary.items())) + 1,300,ignore_label =-1)\n",
    "            self.small_linear_str1 = L.Linear(small_str_source_len * 300,100)\n",
    "            #self.small_batch_str = L.BatchNormalization(500)\n",
    "             \n",
    "            self.small_embed_int = L.EmbedID(int(len(small_int_dictionary.items())) + 1,300,ignore_label =-1)\n",
    "            self.small_linear_int1 = L.Linear(small_int_source_len * 300,100)\n",
    "            #self.small_batch_int = L.BatchNormalization(500)\n",
    "            for i in range(big_str_source_len): \n",
    "                self.add_link('embed_str' + str(i),L.EmbedID(int(len(big_str_dictionary[i].items())) + 1,300,ignore_label =-1))\n",
    "                self.add_link('linear_str' + str(i),L.Linear(300,100))\n",
    "                #self.add_link('linear_str'+str(i),L.Linear((int(len(str_dictionary[i].items()))*\n",
    "                #100),50))\n",
    "                #self.add_link('batch_str' + str(i), L.BatchNormalization(500))\n",
    "            for i in range(big_int_source_len): \n",
    "                self.add_link('embed_int' + str(i),L.EmbedID(int(len(big_int_dictionary[i].items())) + 1,300,ignore_label =-1))\n",
    "                self.add_link('linear_int' + str(i),L.Linear(300,100))\n",
    "                #self.add_link('batch_int' + str(i), L.BatchNormalization(500))\n",
    "\n",
    "\n",
    "            self.linear_float = L.Linear(float_source_len,100)\n",
    "            #self.batch__float = L.BatchNormalization(500)\n",
    "\n",
    "            self.L1 = L.Linear(big_int_source_len*100 +big_str_source_len * 100 + 100 + 100 + 100 , 4000)\n",
    "            self.L2 = L.Linear(4000, 4000)\n",
    "            self.L3 = L.Linear(4000, 2)\n",
    "            self.small_str_source_len = small_str_source_len\n",
    "            self.small_int_source_len = small_int_source_len\n",
    "            self.big_str_source_len = big_str_source_len\n",
    "            self.big_int_source_len = big_int_source_len\n",
    "            self.float_source_len = float_source_len\n",
    "    def forward(self,mini_small_data_str,mini_small_data_int,mini_big_data_str,mini_big_data_int,mini_data_float,ans_data):\n",
    "        mini_small_data_str = xp.array(mini_small_data_str,dtype=xp.int32)\n",
    "        mini_small_data_int = xp.array(mini_small_data_int,dtype=xp.int32)\n",
    "        mini_big_data_str = xp.array(mini_big_data_str,dtype=xp.int32)\n",
    "        mini_big_data_int = xp.array(mini_big_data_int,dtype=xp.int32)\n",
    "        mini_data_float = xp.array(mini_data_float,dtype=xp.float32)\n",
    "        ys = xp.array(ans_data,dtype=xp.int32)\n",
    "\n",
    "        h1 = Variable()\n",
    "        h2 = Variable()\n",
    "        h3 = Variable()\n",
    "        for i in range(self.big_str_source_len): \n",
    "            if i == 0:\n",
    "                em2h_0 = self['embed_str' + str(i)](mini_big_data_str[i])\n",
    "                h1 = self['linear_str' + str(i)](em2h_0)\n",
    "                #h1 = F.relu(self['batch_str' + str(i)](em2h_1))\n",
    "                \n",
    "            else:\n",
    "                em2h_0 = self['embed_str' + str(i)](mini_big_data_str[i])\n",
    "                em2h_2 = self['linear_str' + str(i)](em2h_0)\n",
    "                #em2h_2 = F.relu(self['batch_str' + str(i)](em2h_1))\n",
    "                h1 = F.concat((h1, em2h_2), axis=1)\n",
    "        for i in range(self.big_int_source_len): \n",
    "            if i == 0:\n",
    "                em2h_0 = self['embed_int' + str(i)](mini_big_data_int[i])\n",
    "                h2 = self['linear_int' + str(i)](em2h_0)\n",
    "                #h2 = F.relu(self['batch_int' + str(i)](em2h_1))\n",
    "                \n",
    "            else:\n",
    "                em2h_0 = self['embed_int' + str(i)](mini_big_data_int[i])\n",
    "                em2h_2 = self['linear_int' + str(i)](em2h_0)\n",
    "                #em2h_2 = F.relu(self['batch_int' + str(i)](em2h_1))\n",
    "                h2 = F.concat((h2, em2h_2), axis=1)\n",
    "            pass\n",
    "        h1_1 = self.small_embed_str(mini_small_data_str)\n",
    "        h1_1 = F.relu(self.small_linear_str1(h1_1) )\n",
    "        #h1_1 = F.relu(self.small_batch_str(h1_1)) \n",
    "\n",
    "        h2_1 = self.small_embed_int(mini_small_data_int)\n",
    "        h2_1 = F.relu(self.small_linear_int1(h2_1)) \n",
    "        #h2_1 = F.relu(self.small_batch_int(h2_1)) \n",
    "\n",
    "        h3 = self.linear_float(mini_data_float)\n",
    "        #h3 = self.batch__float(h3)\n",
    "\n",
    "        h4 = F.concat((h1, h2), axis=1) \n",
    "        h4 = F.concat((h4, h1_1), axis=1)\n",
    "        h4 = F.concat((h4, h2_1), axis=1)\n",
    "        h4 = F.concat((h4, h3), axis=1)\n",
    "        h5 = self.L1(h4)\n",
    "        h5 =  self.L2(h5)\n",
    "        y = self.L3(h5)\n",
    "        loss = F.softmax_cross_entropy(y, ys) \n",
    "        acc = F.accuracy(y, ys)\n",
    "        report({'accuracy': acc.data}, self)\n",
    "        report({'loss': loss.data}, self)\n",
    "        return loss  \n",
    "#https://www.kaggle.com/theoviel/load-the-totality-of-the-data\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "def non_cache():\n",
    "    dir1 = '../input/vvwwww1/'\n",
    "    project_name = 'proc2_'\n",
    "    print(\"start training\")\n",
    "    floats = ['float16', 'float32', 'float64']\n",
    "    ints = ['int8', 'int16', 'int32', 'int64']\n",
    "    category = ['float16', 'float32', 'float64','int8', 'int16', 'int32', 'int64']\n",
    "    float_columns = [c for c,v in dtypes.items() if v in floats]\n",
    "    int_columns = [c for c,v in dtypes.items() if v in ints]\n",
    "    categorical_columns = [c for c,v in dtypes.items() if v not in category]\n",
    "\n",
    "    proc_data = pd.read_csv(r\"U:\\ms\\train.csv\")  \n",
    "    proc_data = proc_data.drop('MachineIdentifier', axis=1)\n",
    "    y = proc_data['HasDetections'].values.tolist()  \n",
    "    proc_data = proc_data.drop('HasDetections', axis=1) \n",
    "    proc_data = reduce_mem_usage(proc_data)\n",
    "\n",
    "\n",
    "    stats = []\n",
    "    for col in proc_data.columns:\n",
    "        stats.append((col, proc_data[col].nunique(), proc_data[col].isnull().sum() * 100 / proc_data.shape[0], proc_data[col].value_counts(normalize=True, dropna=False).values[0] * 100, proc_data[col].dtype))\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\n",
    "    stats_df.sort_values('Percentage of missing values', ascending=False)\n",
    " \n",
    "    good_cols = list(proc_data.columns)\n",
    "    for col in proc_data.columns:\n",
    "        rate = proc_data[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "        if rate > 0.9:\n",
    "            good_cols.remove(col)\n",
    "    float_columns1 = []\n",
    "    int_columns1 = []\n",
    "    categorical_columns1 = []\n",
    "    for item in float_columns:\n",
    "        if item in good_cols:\n",
    "            float_columns1.append(item)\n",
    "        pass\n",
    "    for item in int_columns:\n",
    "        if item  in good_cols:\n",
    "            int_columns1.append(item)\n",
    "        pass\n",
    "    for item in categorical_columns:\n",
    "        if item in good_cols:\n",
    "            categorical_columns1.append(item)\n",
    "            \n",
    "    small_int_columns = []\n",
    "    small_str_columns = []\n",
    "    big_int_columns = []\n",
    "    big_str_columns = []\n",
    "\n",
    "    for item in int_columns1:\n",
    "        if stats_df[stats_df.Feature ==item]['Unique_values'].values > 100:\n",
    "            big_int_columns.append(item)\n",
    "        else:\n",
    "            small_int_columns.append(item)\n",
    "    for item in categorical_columns1:\n",
    "        if stats_df[stats_df.Feature ==item]['Unique_values'].values > 100:\n",
    "            big_str_columns.append(item)\n",
    "        else:\n",
    "            small_str_columns.append(item)\n",
    "\n",
    "    proc_data = proc_data[good_cols] \n",
    "\n",
    "    small_str_source =[item1 for item1 in proc_data[small_str_columns].fillna('-1').values.tolist()]\n",
    "    small_str_dictionary = corpora.Dictionary(small_str_source) \n",
    "    small_str_dictionary = dict([[key, value1 ] for key, value1 in small_str_dictionary.token2id.items()])\n",
    "    small_str_dictionary['-1'] = -1\n",
    "    small_str_source =Parallel_process_source_dic.main_proc_1(small_str_source,small_str_dictionary)\n",
    "\n",
    "    big_str_dictionary =[]\n",
    "    str_int = -1\n",
    "    big_str_source =[]\n",
    "    big_str_source =[]\n",
    "    for item in big_str_columns:\n",
    "        str_int = str_int +1\n",
    "        source = [[item1] for item1 in proc_data[item].fillna('-1').values.tolist()]\n",
    "        big_str_dictionary.append(corpora.Dictionary(source))\n",
    "        big_str_dictionary[str_int] = dict([[key, value1 ] for key, value1 in big_str_dictionary[str_int].token2id.items()])\n",
    "        big_str_dictionary[str_int]['-1'] = -1\n",
    "        big_str_source.append(Parallel_process_source_dic.main_proc_1(source,big_str_dictionary[str_int]))\n",
    "        print(item +':' +str(len(big_str_dictionary[str_int].items())))\n",
    "\n",
    "    small_int_source =[[str(v) for v in item1] for item1 in proc_data[small_int_columns].fillna('-1').values.tolist()]\n",
    "    small_int_dictionary = corpora.Dictionary(small_int_source) \n",
    "    small_int_dictionary = dict([[key, value1 ] for key, value1 in small_int_dictionary.token2id.items()])\n",
    "    small_int_dictionary['-1'] = -1\n",
    "    small_int_source =Parallel_process_source_dic.main_proc_1(small_int_source,small_int_dictionary)\n",
    "\n",
    "    big_int_dictionary =[]\n",
    "    int_int = -1\n",
    "    big_int_source =[]\n",
    "    for item in big_int_columns:\n",
    "        int_int = int_int +1\n",
    "        source = [[str(item1)] for item1 in proc_data[item].fillna('-1').values.tolist()]\n",
    "        big_int_dictionary.append(corpora.Dictionary(source))\n",
    "        big_int_dictionary[int_int] = dict([[key, value1 ] for key, value1 in big_int_dictionary[int_int].token2id.items()])\n",
    "        big_int_dictionary[int_int]['-1'] = -1\n",
    "        big_int_source.append(Parallel_process_source_dic.main_proc_1(source,big_int_dictionary[int_int]))\n",
    "        print(item +':'+str(len(big_int_dictionary[int_int].items())))\n",
    "    float_source =[] \n",
    "   \n",
    "    float_source = [item1 for item1 in proc_data[float_columns1].fillna(0).values.tolist()]\n",
    "        \n",
    "    \n",
    "    small_str_source_len = len(small_str_source[0])\n",
    "    big_str_source_len = len(big_str_source)\n",
    "    small_int_source_len = len(small_int_source[0])\n",
    "    big_int_source_len = len(big_int_source)\n",
    "    float_source_len =len(float_source[0])\n",
    "\n",
    "    small_str_source = np.array(small_str_source,dtype=np.int32)\n",
    "    small_int_source = np.array(small_int_source,dtype=np.int32)\n",
    "    big_str_source = np.array(big_str_source,dtype=np.int32)\n",
    "    big_int_source = np.array(big_int_source,dtype=np.int32)\n",
    "    float_source = np.array(float_source,dtype=np.float32)\n",
    "    with open(dir1 + project_name + 'small_str_dictionary.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(small_str_dictionary, f)\n",
    "    with open(dir1 + project_name + 'small_int_dictionary.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(small_int_dictionary, f)\n",
    "    with open(dir1 + project_name + 'big_str_dictionary.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(big_str_dictionary, f)\n",
    "    with open(dir1 + project_name + 'big_int_dictionary.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(big_int_dictionary, f)\n",
    "    #len\n",
    "    with open(dir1 + project_name + 'small_str_source_len.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(small_str_source_len, f)\n",
    "\n",
    "    with open(dir1 + project_name + 'big_str_source_len.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(big_str_source_len, f)\n",
    "\n",
    "    with open(dir1 + project_name + 'small_int_source_len.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(small_int_source_len, f)\n",
    "\n",
    "    with open(dir1 + project_name + 'big_int_source_len.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(big_int_source_len, f)\n",
    "\n",
    "    with open(dir1 + project_name + 'float_source_len.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(float_source_len, f)\n",
    "    #data\n",
    "    with open(dir1 + project_name + 'small_str_source.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(small_str_source, f)\n",
    "\n",
    "    with open(dir1 + project_name + 'small_int_source.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(small_int_source, f)\n",
    "\n",
    "    with open(dir1 + project_name + 'big_str_source.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(big_str_source, f)\n",
    "\n",
    "    with open(dir1 + project_name + 'big_int_source.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(big_int_source, f)\n",
    "\n",
    "    with open(dir1 + project_name + 'float_source.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(float_source, f)\n",
    "    with open(dir1 + project_name + 'y.pickle', 'wb') as f:  # save cache\n",
    "        pickle.dump(y, f)\n",
    "\n",
    "    model = ms_malware(small_str_source_len,\n",
    "                        small_int_source_len,\n",
    "                        big_str_source_len,\n",
    "                        big_int_source_len,\n",
    "                        float_source_len,\n",
    "                        small_str_dictionary,\n",
    "                        small_int_dictionary,\n",
    "                        big_str_dictionary,\n",
    "                        big_int_dictionary)\n",
    "    \n",
    "    if gpu >= 0:\n",
    "        model.to_gpu()\n",
    "\n",
    "    # Set up an optimizer\n",
    "    optimizer = O.Adam()\n",
    "    optimizer.setup(model)\n",
    "    batchsize = 2046\n",
    "    epoch =20\n",
    "    gpu = 0\n",
    "    # Set up an iterator\n",
    "    train_iter = WindowIterator(small_str_source=small_str_source,\n",
    "                                 small_int_source=small_int_source,\n",
    "                                 big_str_source = big_str_source,\n",
    "                                 big_int_source = big_int_source,\n",
    "                                 float_source=float_source,\n",
    "                                 ans=y,\n",
    "                                batch_size=batchsize, \n",
    "                                repeat=True)\n",
    "    \n",
    "    # Set up an updater\n",
    "    updater = training.updaters.StandardUpdater(\n",
    "        train_iter, optimizer, converter=convert, device=gpu)\n",
    "\n",
    "    # Set up a trainer\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=\"result\")\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'validation/main/loss', 'main/accuracy', 'validation/main/accuracy']))\n",
    "    trainer.extend(extensions.ProgressBar())\n",
    "    trainer.run()\n",
    "    model.to_cpu()\n",
    "    serializers.save_npz(dir1 + project_name + '.model', model)\n",
    "    serializers.save_npz(dir1 + project_name + '.state', optimizer)\n",
    "    pass\n",
    "def yes_cache():\n",
    "    gpu = 0\n",
    "    dir1 = '../input/vvwwww1/'\n",
    "    project_name = 'proc2_'\n",
    "    with open(dir1 + project_name + 'small_str_dictionary.pickle', 'rb') as f: #load cache\n",
    "        small_str_dictionary = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'small_int_dictionary.pickle', 'rb') as f: #load cache\n",
    "        small_int_dictionary = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'big_str_dictionary.pickle', 'rb') as f: #load cache\n",
    "        big_str_dictionary = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'big_int_dictionary.pickle', 'rb') as f: #load cache\n",
    "        big_int_dictionary = pickle.load(f)\n",
    "    #len\n",
    "\n",
    "    with open(dir1 + project_name + 'small_str_source_len.pickle', 'rb') as f: #load cache\n",
    "        small_str_source_len = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'big_str_source_len.pickle', 'rb') as f: #load cache\n",
    "        big_str_source_len = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'small_int_source_len.pickle', 'rb') as f: #load cache\n",
    "        small_int_source_len = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'big_int_source_len.pickle', 'rb') as f: #load cache\n",
    "        big_int_source_len = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'float_source_len.pickle', 'rb') as f: #load cache\n",
    "        float_source_len = pickle.load(f)\n",
    "    #data  \n",
    "    with open(dir1 + project_name + 'small_str_source.pickle', 'rb') as f: #load cache\n",
    "        small_str_source = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'small_int_source.pickle', 'rb') as f: #load cache\n",
    "        small_int_source = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'big_str_source.pickle', 'rb') as f: #load cache\n",
    "        big_str_source = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'big_int_source.pickle', 'rb') as f: #load cache\n",
    "        big_int_source = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'small_str_source.pickle', 'rb') as f: #load cache\n",
    "        small_str_source = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'float_source.pickle', 'rb') as f: #load cache\n",
    "        float_source = pickle.load(f)\n",
    "    with open(dir1 + project_name + 'y.pickle', 'rb') as f: #load cache\n",
    "        y = pickle.load(f)\n",
    "         \n",
    "    model = ms_malware(small_str_source_len,\n",
    "                        small_int_source_len,\n",
    "                        big_str_source_len,\n",
    "                        big_int_source_len,\n",
    "                        float_source_len,\n",
    "                        small_str_dictionary,\n",
    "                        small_int_dictionary,\n",
    "                        big_str_dictionary,\n",
    "                        big_int_dictionary )\n",
    "    \n",
    "    if gpu >= 0:\n",
    "       model.to_gpu()\n",
    "\n",
    "    # Set up an optimizer\n",
    "    optimizer = O.Adam()\n",
    "    optimizer.setup(model)\n",
    "    batchsize =2046\n",
    "    epoch =20\n",
    "    gpu = 0\n",
    "    # Set up an iterator\n",
    "    train_iter = WindowIterator(small_str_source=small_str_source,\n",
    "                                 small_int_source=small_int_source,\n",
    "                                 big_str_source = big_str_source,\n",
    "                                 big_int_source = big_int_source,\n",
    "                                 float_source=float_source,\n",
    "                                 ans=y,\n",
    "                                batch_size=batchsize, \n",
    "                                repeat=True)\n",
    "    \n",
    "    # Set up an updater\n",
    "    updater = training.updaters.StandardUpdater(\n",
    "        train_iter, optimizer, converter=convert, device=gpu)\n",
    "\n",
    "    # Set up a trainer\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=\"result\")\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'validation/main/loss', 'main/accuracy', 'validation/main/accuracy']))\n",
    "    #trainer.extend(extensions.ProgressBar())\n",
    "    trainer.run()\n",
    "    model.to_cpu()\n",
    "    serializers.save_npz('1.model', model)\n",
    "    serializers.save_npz('1.state', optimizer)\n",
    "    pass\n",
    "if __name__ =='__main__':\n",
    "    #non_cache()\n",
    "    yes_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
